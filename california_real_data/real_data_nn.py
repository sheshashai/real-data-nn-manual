# -*- coding: utf-8 -*-
"""real_data_nn.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kfsBXoSEnYPf7iriTX6a76VEKXeS1uIp
"""

import torch
from sklearn.datasets import fetch_california_housing
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Load dataset
data = fetch_california_housing()
X = data.data       # shape: (20640, 8)
y = data.target     # shape: (20640,)

# Normalize features
scaler = StandardScaler()
X = scaler.fit_transform(X)

# Convert to PyTorch tensors
X = torch.tensor(X, dtype=torch.float32)
y = torch.tensor(y, dtype=torch.float32).unsqueeze(1)  # shape: (N, 1)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Use only a small batch to start
X_batch = X_train[:32]
y_batch = y_train[:32]

import numpy as np

torch.manual_seed(42)

input_size=8
hidden_size=3
output_size=1
learning_rate=0.01
epochs=100

# Initialize weights and biases
W1 = torch.randn(hidden_size, input_size, dtype=torch.float32) * 0.1  # (3, 8)
b1 = torch.zeros(hidden_size, dtype=torch.float32)                    # (3,)
W2 = torch.randn(output_size, hidden_size, dtype=torch.float32) * 0.1  # (1, 3)
b2 = torch.zeros(output_size, dtype=torch.float32)                    # (1,)

# ReLU and its gradient
def relu(z):
    return torch.maximum(z, torch.tensor(0.0))

def relu_grad(z):
    return (z > 0).float()

# MSE Loss
def mse_loss(y_pred, y_true):
    return torch.mean((y_pred - y_true) ** 2)

# Training loop
loss_history = []
batch_size = X_batch.shape[0]

for epoch in range(epochs):
    # Forward pass
    Z1 = X_batch @ W1.t() + b1
    A1 = relu(Z1)
    Z2 = A1 @ W2.t() + b2
    y_pred = Z2

    # Loss
    loss = mse_loss(y_pred, y_batch)
    loss_history.append(loss.item())

    # Backword pass (Manual Gradients)
    dL_dy_hat = 2 * (y_pred - y_batch) / batch_size     # (32, 1)
    dL_dW2 = dL_dy_hat.T @ A1                           # (1, 3)
    dL_db2 = torch.sum(dL_dy_hat, dim=0)                # (1,)

    dL_dA1 = dL_dy_hat @ W2                             # (32, 3)
    dL_dZ1 = dL_dA1 * relu_grad(Z1)                     # (32, 3)
    dL_dW1 = dL_dZ1.T @ X_batch                         # (3, 8)
    dL_db1 = torch.sum(dL_dZ1, dim=0)                   # (3,)

    # -------- Update Weights --------
    W2 -= learning_rate * dL_dW2
    b2 -= learning_rate * dL_db2
    W1 -= learning_rate * dL_dW1
    b1 -= learning_rate * dL_db1

    if epoch % 10 == 0 or epoch == epochs - 1:
        print(f"Epoch {epoch+1:3d} - Loss: {loss.item():.4f}")

# Plot the loss
import matplotlib.pyplot as plt
plt.plot(loss_history)
plt.title("Training Loss (MSE)")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.grid(True)
plt.show()

# Use the trained model on test data
Z1_test = X_test @ W1.T + b1
A1_test = relu(Z1_test)
y_test_pred = A1_test @ W2.T + b2

# Compare predictions vs true values (first 10)
for i in range(10):
    print(f"Predicted: {y_test_pred[i].item():.2f} â€” Actual: {y_test[i].item():.2f}")

plt.plot(loss_history)
plt.title("Training Loss on California Housing")
plt.xlabel("Epoch")
plt.ylabel("MSE Loss")
plt.grid(True)
plt.savefig("california_loss_plot.png")
plt.show()